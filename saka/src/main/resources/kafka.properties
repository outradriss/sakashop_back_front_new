## ===========================
## 1. Kafka Broker Configuration
## ===========================
#
## Adresse du broker Kafka. C'est l'endroit où les producteurs et consommateurs se connectent pour lire ou écrire des messages.
#spring.kafka.bootstrap-servers=localhost:9092
#
## Identifiant unique du producteur Kafka pour identifier ce client auprès du broker.
#spring.kafka.producer.client-id=orders-producer
#
## Identifiant unique du consommateur Kafka pour identifier ce client auprès du broker.
#spring.kafka.consumer.client-id=orders-items-consumer
#
## Protocole de sécurité pour la communication (PLAINTEXT signifie que SSL est désactivé).
#spring.kafka.properties.security.protocol=PLAINTEXT
#
## ===========================
## 2. Producer Configuration
## ===========================
#
## Sérialiseur pour les clés des messages. Il convertit les clés en format `String`.
#spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
#
## Sérialiseur pour les valeurs des messages. Ici, on utilise JSON pour sérialiser les objets Java.
#spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer
#
## Niveau d'accusé de réception : "all" signifie que toutes les répliques doivent confirmer l'écriture.
#spring.kafka.producer.acks=all
#
## Taille maximale d'un message produit en octets (1 Mo ici).
#spring.kafka.producer.max-request-size=1048576
#
## Nombre de tentatives de renvoi d'un message en cas d'échec temporaire.
#spring.kafka.producer.retries=3
#
## Temps maximum pour recevoir un accusé de réception (en millisecondes).
#spring.kafka.producer.request-timeout-ms=15000
#
## Taille d'un lot de messages. Plus grand le batch, meilleure est la performance pour de gros volumes.
#spring.kafka.producer.batch-size=16384
#
## Temps d'attente pour remplir le batch avant l'envoi (en millisecondes).
#spring.kafka.producer.linger-ms=10
#
## Compression des messages pour optimiser l'utilisation du réseau (gzip est utilisé ici).
#spring.kafka.producer.compression-type=gzip
#
## ===========================
## 3. Consumer Configuration
## ===========================
#
## Désérialiseur pour les clés des messages. Il convertit les clés du format `String` en objet Java.
#spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#
## Désérialiseur pour les valeurs des messages. Il convertit le JSON en objets Java.
#spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer
#
## Identifiant du groupe de consommateurs. Les consommateurs dans le même groupe partagent les partitions.
#spring.kafka.consumer.group-id=orders-groupes
#
## Comportement du consommateur en cas d'absence d'offset : "earliest" commence depuis le début des messages.
#spring.kafka.consumer.auto-offset-reset=earliest
#
## Nombre maximum d'enregistrements lus lors d'une seule opération `poll()`.
#spring.kafka.consumer.max-poll-records=500
#
## Temps d'attente maximum pour recevoir des messages lors d'une opération `poll()`.
#spring.kafka.consumer.poll-timeout=3000
#
## Intervalle entre les messages heartbeat pour maintenir la session active.
#spring.kafka.consumer.heartbeat-interval=3000
#
## Timeout pour détecter une défaillance de la session du consommateur.
#spring.kafka.consumer.session-timeout-ms=10000
#
## ===========================
## 4. SSL Security Configuration
## ===========================
#
## Propriétés SSL désactivées ici. Elles sont commentées car le protocole PLAINTEXT est utilisé.
## spring.kafka.ssl.truststore-location=file:C:/Users/bouda/Downloads/SSL/kafka.truststore.jks
## spring.kafka.ssl.truststore-password=${truststore-password}
## spring.kafka.ssl.keystore-location=file:C:/Users/bouda/Downloads/SSL/kafka.keystore.jks
## spring.kafka.ssl.keystore-password=${keystore-password}
## spring.kafka.ssl.key-password=key-password
#
## ===========================
## 5. Topic Configuration
## ===========================
#
## Nom du topic Kafka où les données sont publiées.
#spring.kafka.topic.orders.name=mydb.sakashop.orders
#
## Nombre de partitions du topic. Plus il y a de partitions, plus le traitement parallèle est efficace.pour gérer le traitement en parallèle
#spring.kafka.topic.orders.partitions=3
#
## Facteur de réplication pour garantir la haute disponibilité des données dans Kafka.
#spring.kafka.topic.orders.replication-factor=3
#
## ===========================
## 6. Kafka Streams Configuration
## ===========================
#
## Identifiant unique pour l'application Kafka Streams.
#spring.kafka.streams.application-id=sakashop-streams-app
#
## Démarrage automatique de Kafka Streams lors du démarrage de l'application Spring Boot.
#spring.kafka.streams.auto-startup=true
#
## Nettoyage de l'état local des flux Kafka au démarrage.
#spring.kafka.streams.cleanup-on-startup=true
#
## Facteur de réplication des états internes de Kafka Streams.
#spring.kafka.streams.replication-factor=1
#
## Sérialiseur par défaut pour les clés des messages dans Kafka Streams.
#spring.kafka.streams.default-key-serde=org.apache.kafka.common.serialization.Serdes$StringSerde
#
## Sérialiseur par défaut pour les valeurs des messages dans Kafka Streams.
#spring.kafka.streams.default-value-serde=org.apache.kafka.common.serialization.Serdes$StringSerde
#
## Garantie du traitement exact des messages (Exactly Once).
#spring.kafka.streams.processing.guarantee=exactly_once
#
## ===========================
## 7. Logs Management
## ===========================
#
## Niveau de log pour Kafka (DEBUG pour obtenir des informations détaillées).
#logging.level.org.apache.kafka=DEBUG
#
## Niveau de log pour les composants Kafka dans Spring.
#logging.level.org.springframework.kafka=DEBUG
#
## Fichier où les logs Kafka seront écrits.
#logging.file.name=kafka-application.log
#
## ===========================
## 8. Manual Offset and Partition Management
## ===========================
#
## Désactive la validation automatique des offsets pour un meilleur contrôle.
#spring.kafka.consumer.enable-auto-commit=false
#
## ===========================
## 9. Monitoring and Metrics
## ===========================
#
## Activer l'exposition des métriques Kafka via Actuator.
#management.endpoint.kafka.enabled=true
#
## Exposer les endpoints d'Actuator pour les métriques.
#management.endpoints.web.exposure.include=*
#
## Activer l'export des métriques vers Prometheus.
#management.metrics.export.prometheus.enabled=true
#
## Timeout pour les sessions de broker.
#broker.session.timeout.ms=6000
#
## Intervalle pour les messages heartbeat entre le broker et les consommateurs.
#broker.heartbeat.interval.ms=2000
